{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOhH6DOd4JX2NL7Aak/46BN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kapibara3chi/NLPTesForGC/blob/main/NLPTest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# pip"
      ],
      "metadata": {
        "id": "NlJEPxsp8FTG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip  install -U ginza ja_ginza_electra\n",
        "!pip install plantuml"
      ],
      "metadata": {
        "id": "_3vltd35PtOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plant UML"
      ],
      "metadata": {
        "id": "eSryLFdubFcW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/sample.puml', 'w+') as f:\n",
        "\tf.write('''\n",
        "\n",
        "@startuml\n",
        "start\n",
        ":Dependency parsing;\n",
        ":Get Nouns;\n",
        ":Get Verbs;\n",
        ":Get NOUN-VERB relations;\n",
        ":Combines nouns;\n",
        ":Get relations between verbs;\n",
        ":Get NOUN-AUX relations;\n",
        ":Get VERB-AUX relations;\n",
        ":Make DataFrame;\n",
        ":Make No link Flag;\n",
        ":Negative-Positive Analysis of Verbs;\n",
        "end\n",
        "@enduml\n",
        "\n",
        "''')\n",
        "\n",
        "!python -m plantuml sample.puml\n",
        "\n",
        "from IPython.display import Image\n",
        "\n",
        "Image('sample.png')"
      ],
      "metadata": {
        "id": "I6NcZfwTazvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code"
      ],
      "metadata": {
        "id": "VM-ayrWDSXHH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "id": "BAHbPrHYSVEh",
        "outputId": "957c09a0-ddb2-420f-cbda-1ab67f38e2ab"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-7db45f57f3df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mginza\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdisplacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ginza'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import ginza\n",
        "from spacy import displacy\n",
        "import spacy\n",
        "import sys\n",
        "import pandas as pd\n",
        "class DependencyAnalysis:\n",
        "    def __init__(self):\n",
        "        self.nlp = spacy.load('ja_ginza_electra')\n",
        "        self.indices = []\n",
        "        self.noun_l=[]\n",
        "        self.verb_l=[]\n",
        "        self.noun_verb_relation_l=[]\n",
        "        self.merged_cnoun_l=[]\n",
        "        self.sent_cnt=0\n",
        "        cols = [\"sent_cnt\",\"i\", \"orth\", \"base\",\"head\",\"dep\",\"pos\",\"tag\"]\n",
        "        # self.dependent_indecies_pd = pd.DataFrame(index=[], columns=cols)\n",
        "        self.depend_indecies_pd = pd.DataFrame(index=[], columns=cols)\n",
        "\n",
        "    def get_analysis(self, text):\n",
        "        \"\"\"係り受け解析\"\"\"\n",
        "        doc = self.nlp(text)\n",
        "\n",
        "        token_head_list = []\n",
        "\n",
        "        for sent in doc.sents:\n",
        "            for token in sent:\n",
        "                token_head_list.append(\n",
        "                    {\"i\": token.i, \"orth\": token.orth_, \"base\": token.lemma_,\n",
        "                     \"head\": token.head.i, \"dep\": token.dep_,\"pos\":token.pos_,\n",
        "                     \"tag\":token.tag_})\n",
        "                df_tmp=pd.DataFrame({\"sent_cnt\":self.sent_cnt,\"i\": [token.i], \n",
        "                                     \"orth\": [token.orth_], \"base\":[token.lemma_],\n",
        "                                     \"head\":[token.head.i], \"dep\": [token.dep_],\n",
        "                                     \"pos\":[token.pos_], \"tag\":[token.tag_]})\n",
        "                self.depend_indecies_pd= pd.concat([self.depend_indecies_pd, df_tmp])\n",
        "\n",
        "        displacy.render(doc, style='dep', jupyter=True, options={'distance': 90})\n",
        "        print(self.depend_indecies_pd)\n",
        "\n",
        "        return token_head_list\n",
        "    # def get(self, text, target):\n",
        "    #   # 係り受け解析\n",
        "    #   self.depend_indices = self.get_analysis(text)\n",
        "    #   print(self.depend_indices)\n",
        "\n",
        "    def get_words(self,  m, head):\n",
        "        \"\"\"対象の領域の抽出\"\"\"\n",
        "        \n",
        "        if m[\"head\"] == head :\n",
        "            for n in self.depend_indices:\n",
        "                self.get_words(m=n, head=m[\"i\"])\n",
        "                            \n",
        "            self.indices.append(m[\"i\"])\n",
        "        \n",
        "        return\n",
        "    def get(self, text, target):\n",
        "        \n",
        "        # 係り受け解析\n",
        "        self.depend_indices = self.get_analysis(text)\n",
        "        # print(self.depend_indices)\n",
        "        for i in self.depend_indices:\n",
        "          print(i)\n",
        "                \n",
        "        # 係り元のインデックスを取得\n",
        "        head = self.depend_indices[target][\"head\"]\n",
        "        # print(head)\n",
        "\n",
        "        # 係り元のかかり先を探索\n",
        "        self.indices.append(head)\n",
        "        print(self.indices)#list of head?\n",
        "        for m in self.depend_indices:\n",
        "            if m[\"head\"] == head and m[\"dep\"] != \"ROOT\":\n",
        "                self.get_words(m, head=head)\n",
        "\n",
        "        clause = \"\"\n",
        "        for clause_num in sorted(self.indices):\n",
        "            clause = clause + str(self.depend_indices[clause_num][\"orth\"])\n",
        "            \n",
        "        return clause\n",
        "\n",
        "\n",
        "\n",
        "    def get_noun(self):\n",
        "      # print(type(self.depend_indices))\n",
        "      # noun_l=[i for i in self.depend_indices if \"名詞\" in i[\"tag\"]]\n",
        "\n",
        "      ex_l=[\"こと\"]\n",
        "      noun_l=[i for i in self.depend_indices if \"NOUN\" in i[\"pos\"] or \"PROPN\" in i[\"pos\"] or \"NUM\" in i[\"pos\"]or \"SYM\" in i[\"pos\"]]\n",
        "      noun_l=[i for i in noun_l if not i[\"base\"] in ex_l]\n",
        "\n",
        "      noun_df=self.depend_indecies_pd[self.depend_indecies_pd[\"sent_cnt\"]==self.sent_cnt & (self.depend_indecies_pd[\"pos\"]==\"NOUN\" | self.depend_indecies_pd[\"pos\"]==\"PROPN\"| self.depend_indecies_pd[\"pos\"]==\"NUM\"| self.depend_indecies_pd[\"pos\"]==\"SYM\") ]\n",
        "      for i in ex_l:\n",
        "        noun_df=noun_df[noun_df[\"base\"]!=i]\n",
        "      \n",
        "\n",
        "      # print(self.noun_df)\n",
        "      self.noun_df_tmp=noun_df\n",
        "      print(\"noun_df_tmp\",self.noun_df_tmp)\n",
        "\n",
        "      self.depend_indecies_pd= pd.concat([self.depend_indecies_pd, df_tmp])\n",
        "      self.noun_df=pd.concat([self.noun_df,self.noun_df_tmp])\n",
        "      print(\"noun_df\",self.noun_df)\n",
        "      return noun_l\n",
        "   \n",
        "    def get_verb(self):\n",
        "      verb_l=[i for i in self.depend_indices if \"VERB\" in i[\"pos\"]]\n",
        "      ex_l=[\"する\",\"おく\"]\n",
        "      verb_l=[i for i in verb_l if not i[\"base\"] in ex_l]\n",
        "      # print(verb_l)\n",
        "\n",
        "      self.verb_df=self.depend_indecies_pd[self.depend_indecies_pd[\"pos\"]==\"VERB\"]\n",
        "      for i in ex_l:\n",
        "        self.verb_df=self.verb_df[self.verb_df[\"base\"]!=i]\n",
        "      # print(self.verb_df)\n",
        "      return verb_l\n",
        "\n",
        "    def seek_verb(self,noun,seek=\"VERB\"): \n",
        "      #Find verb from noun\n",
        "      if noun is None:\n",
        "        return\n",
        "      parents_idx=noun[\"head\"]\n",
        "      if noun[\"i\"]==noun[\"head\"]:\n",
        "        # print(\"same index\")\n",
        "        return\n",
        "      # print(parents_idx)\n",
        "      if self.depend_indices[parents_idx][\"pos\"]==\"VERB\":\n",
        "        # print(\"VERB\",self.depend_indices[parents_idx])\n",
        "        return self.depend_indices[parents_idx]\n",
        "        # print(\"error\")\n",
        "      elif self.depend_indices[parents_idx][\"dep\"]==\"ROOT\":\n",
        "        print(\"ROOT END\") \n",
        "        return\n",
        "      else:\n",
        "        # print(\"loop\")\n",
        "        return self.seek_verb(self.depend_indices[parents_idx])\n",
        "\n",
        "    def seek_word(self,noun,seek): \n",
        "      #Find verb from noun\n",
        "      if noun is None:\n",
        "        return\n",
        "      parents_idx=noun[\"head\"]\n",
        "      if noun[\"i\"]==noun[\"head\"]:\n",
        "        # print(\"same index\")\n",
        "        return\n",
        "      # print(parents_idx)\n",
        "      if self.depend_indices[parents_idx][\"pos\"]==seek:\n",
        "        # print(\"VERB\",self.depend_indices[parents_idx])\n",
        "        return self.depend_indices[parents_idx]\n",
        "        # print(\"error\")\n",
        "      elif self.depend_indices[parents_idx][\"dep\"]==\"ROOT\":\n",
        "        # print(\"ROOT END\") \n",
        "        return\n",
        "      else:\n",
        "        # print(\"loop\")\n",
        "        return self.seek_word(self.depend_indices[parents_idx],seek)\n",
        "\n",
        "    def merge_set(self,set_l):\n",
        "      merged_set_l=[]\n",
        "      if len(set_l)==1:\n",
        "        merged_set_l=set_l\n",
        "        return merged_set_l\n",
        "\n",
        "      for i in range(len(set_l)):\n",
        "        itm=set_l.pop(0)\n",
        "        # print(\"loop\",i)\n",
        "        # print(\"list:\",set_l)\n",
        "        # print(\"i:\",i,\"pop itm:\",itm)\n",
        "        flag=False\n",
        "        # print(\"len(set_l)\",len(set_l))\n",
        "        if len(set_l)==0:\n",
        "          itm_l=list(itm)\n",
        "          itm_l.sort()\n",
        "          merge_word=self.make_compound_word(itm_l)\n",
        "          merged_set_l.append([itm_l,merge_word])\n",
        "          # print(\"len=0 break\")\n",
        "          break\n",
        "        for idx,j in enumerate(set_l):\n",
        "          if len(itm & j) !=0:\n",
        "            mrg=itm | j\n",
        "            set_l[idx]=mrg\n",
        "            flag=True\n",
        "            # print(\"merge:\",mrg)\n",
        "            break\n",
        "        if flag==False:\n",
        "          itm_l=list(itm)\n",
        "          itm_l.sort()\n",
        "          merge_word=self.make_compound_word(itm_l)\n",
        "          merged_set_l.append([itm_l,merge_word])\n",
        "        # print(\"merged list:\",merged_set_l)\n",
        "        # print(\"-\"*10)\n",
        "      # print(\"=\"*100)\n",
        "      # print(merged_set_l)\n",
        "      # print(\"=\"*100)\n",
        "      return merged_set_l\n",
        "\n",
        "    def make_compound_word(self,merged_cnoun_l):\n",
        "      cnoun=\"\"\n",
        "      for i in merged_cnoun_l:\n",
        "        # print(\"i\",i)\n",
        "        for l in self.depend_indices:\n",
        "          if l[\"i\"]==i:\n",
        "            cnoun+=l[\"orth\"]\n",
        "      # print(cnoun)\n",
        "      return cnoun\n",
        "\n",
        "    def make_compound_nouns(self):\n",
        "      # print(self.depend_indices)\n",
        "      # find compound and nummod\n",
        "      cnoun_l=[[i,self.depend_indices[i[\"head\"]]] for i in self.depend_indices if i[\"dep\"]==\"compound\" or i[\"dep\"]==\"nummod\"]\n",
        "\n",
        "      #find compound index\n",
        "      merge_cnoun_l=[{m[\"i\"] for m in l} for l in cnoun_l]\n",
        "      # print(\"merge_cnoun_l:\",merge_cnoun_l)\n",
        "\n",
        "      #merge noun\n",
        "      merged_cnoun_l=[]\n",
        "      # merge_cnoun_l=[{0,1}]\n",
        "      if len(merge_cnoun_l)==1:\n",
        "        # print(\"merge cnoun_l=1:\",merge_cnoun_l)\n",
        "        itm_l=list(merge_cnoun_l[0])\n",
        "        itm_l.sort()\n",
        "        merge_word=self.make_compound_word(itm_l)\n",
        "        merged_cnoun_l.append([itm_l,merge_word])\n",
        "      else:\n",
        "        merged_cnoun_l=self.merge_set(merge_cnoun_l)\n",
        "        # print(\"merged cnoun_l:\",merged_cnoun_l)\n",
        "\n",
        "      return merged_cnoun_l\n",
        "\n",
        "\n",
        "      \n",
        "    def seek_noun_all(self,noun,cnoun_l):\n",
        "      #Find noun from noun\n",
        "      parents_idx=noun[\"head\"]\n",
        "      # print(parents_idx)\n",
        "\n",
        "      if noun[\"dep\"]==\"compound\" or noun[\"dep\"]==\"nummod\":\n",
        "        cnoun_l.append(self.depend_indices[parents_idx])\n",
        "      else:\n",
        "        return cnoun_l\n",
        "\n",
        "      '''\n",
        "      if self.depend_indices[parents_idx][\"pos\"]==\"NOUN\" or self.depend_indices[parents_idx][\"pos\"]==\"PROPN\" or self.depend_indices[parents_idx][\"pos\"]==\"NUM\":\n",
        "        cnoun_l.append(self.depend_indices[parents_idx])\n",
        "        # print(\"VERB\",self.depend_indices[parents_idx])\n",
        "        return self.seek_noun_all(self.depend_indices[parents_idx],cnoun_l)\n",
        "        # print(\"error\")\n",
        "      else:\n",
        "        return cnoun_l\n",
        "      '''\n",
        "    def get_noun_verb_rel(self):\n",
        "        noun_verb_rel_l=[]\n",
        "        for i in self.noun_l:\n",
        "          # print(\"noun:\",i)\n",
        "          verb=self.seek_verb(i)\n",
        "          # print(\"verb:\",verb)\n",
        "          tmp=[i,verb]\n",
        "          noun_verb_rel_l.append(tmp)\n",
        "\n",
        "        self.noun_verb_relation_l=noun_verb_rel_l\n",
        "\n",
        "    def get_unique_list(self,seq):\n",
        "      #Remove duplicates from the list\n",
        "        seen = []\n",
        "        return [x for x in seq if x not in seen and not seen.append(x)]\n",
        "\n",
        "    def get_verb_verb_rel(self):\n",
        "        verb_verb_l=[]\n",
        "        for i in self.noun_verb_relation_l:\n",
        "          prt_idx=self.seek_verb(i[1])\n",
        "          # print(\"verb:\",i[1])\n",
        "          # print(\"prt_verb:\",prt_idx)\n",
        "          if prt_idx is not None:\n",
        "            tmp_l=[i[1][\"i\"],prt_idx[\"i\"]]\n",
        "            verb_verb_l.append(tmp_l)\n",
        "        # print(verb_verb_l)\n",
        "        return self.get_unique_list(verb_verb_l)\n",
        "        \n",
        "    def get_rel(self,word_l,seek_str):\n",
        "        rel_l=[]\n",
        "        for i in word_l:\n",
        "          # print(\"noun:\",i)\n",
        "          verb=self.seek_word(i,seek_str)\n",
        "          # print(\"verb:\",verb)\n",
        "          if verb is not None:\n",
        "            tmp=[i[\"i\"],verb]\n",
        "            rel_l.append(tmp)\n",
        "        return rel_l\n",
        "\n",
        "    def get_rel_prt_df(self,word_df,seek_str):\n",
        "      # print(type(word_df))\n",
        "      print(word_df)\n",
        "      sent_df=self.depend_indecies_pd[self.depend_indecies_pd[\"sent_cnt\"]==self.sent_cnt]\n",
        "      print(sent_df)\n",
        "      # '''\n",
        "      # print(int(word_df[(word_df[\"i\"]==5)][\"i\"]))\n",
        "      for itm  in word_df.iterrows():\n",
        "        # print(type(itm))\n",
        "        # print(itm)\n",
        "        print(\"i:\",itm[1].i)\n",
        "        print(\"head:\",itm[1][\"head\"])\n",
        "        # print(\"head:\",itm[1].head)\n",
        "        print(\"df:\",sent_df[(sent_df[\"i\"]==itm[1][\"head\"])][\"pos\"].iloc[-1])\n",
        "        # print(\"df:\",len(word_df[(word_df[\"i\"]==itm[1][\"head\"])][\"i\"]))\n",
        "        # print(len(word_df[(word_df[\"i\"]==itm[1][\"head\"])][\"i\"])!=0)\n",
        "      # '''\n",
        "      print(\"-\"*10)\n",
        "      # rel_l=[[i[1][\"i\"],i[1][\"head\"]] for i in word_df.iterrows() if word_df[\"i\"]==[\"pos\"] ]\n",
        "      # print(rel_l)\n",
        "      # return rel_l\n",
        "\n",
        "    def dep_analysis_main(self,text):\n",
        "        # 係り受け解析\n",
        "        self.depend_indices = self.get_analysis(text)\n",
        "\n",
        "        # print(self.depend_indices)\n",
        "        # for i in self.depend_indices:\n",
        "        #   print(i)\n",
        "        # print(\"-\"*100)\n",
        "\n",
        "        #get NOUN\n",
        "        self.noun_l=self.get_noun()\n",
        "        for l in self.noun_l:\n",
        "          print(l)\n",
        "\n",
        "        #get VERB\n",
        "        self.verb_l=self.get_verb()\n",
        "        # for l in verb_l:\n",
        "        #   print(l)\n",
        "\n",
        "        #get NOUN-VERB relations\n",
        "        self.get_noun_verb_rel()\n",
        "        for i in self.noun_verb_relation_l:\n",
        "          print(\"noun:\",i[0])\n",
        "          print(\"verb:\",i[1])\n",
        "        print(\"-\"*100)\n",
        "        \n",
        "        #get compound nouns\n",
        "        # for i in noun_l:\n",
        "          # c_cnouns_l=self.make_compound_nouns(i)\n",
        "        # print(noun_l[0])\n",
        "        self.merged_cnoun_l=self.make_compound_nouns()\n",
        "        print(\"merged cnoun list:\",self.merged_cnoun_l)\n",
        "\n",
        "        #get VERB-VERB relations\n",
        "        verb_verb_l=self.get_verb_verb_rel()\n",
        "        print(\"verb-verb list:\",verb_verb_l)\n",
        "\n",
        "        #Get NOUN-AUX relations;\n",
        "        self.noun_aux_rel=self.get_rel(self.noun_l,\"AUX\")\n",
        "        print(\"noun-aux list:\",self.noun_aux_rel)\n",
        "\n",
        "        #Get VERB-AUX relations;\n",
        "        self.get_rel_prt_df(self.noun_df,\"VERB\")\n",
        "\n",
        "        #Make DataFrame;\n",
        "        \n",
        "        #Make No link Flag;\n",
        "\n",
        "        #Negative-Positive Analysis of Verbs;\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  dependency = DependencyAnalysis()\n",
        "  txt_l=[\"下流あごを2mmだすと分離は改善するが、コンタミが悪化する。\",\n",
        "         \"非通紙部昇温時のxx75において発生する。\",\n",
        "         \"下流あごを出すこととオフセットを0.2mmにすることより解決する\",\n",
        "         \"上流あごは分離に影響を与えない。オフセットの増加は分離に関係ない\",\n",
        "         \"下流あごの増加による分離悪化を説明できない\",\n",
        "         \"下流あごと分離は無関係である\",\n",
        "         \"オフセットと分離は関係ある\",\n",
        "         \"上流オフセットでは分離は改善しない\"]\n",
        "  # ret=dependency.get_analysis(\"下流あごをだすと分離は改善するが、コンタミが悪化する\")\n",
        "  # for i in ret:\n",
        "    # print(i)\n",
        "  for txt in txt_l:\n",
        "    # ret=dependency.get(txt,0)\n",
        "    # print(ret)\n",
        "    dependency.sent_cnt+=1\n",
        "    dependency.dep_analysis_main(txt)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fvf-g8W_7mbV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}